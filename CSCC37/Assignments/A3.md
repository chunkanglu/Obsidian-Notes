# Q1
## 1a
![[Drawing 2023-11-13 18.58.15.excalidraw]]
Since $f(-1) = f(1) = \frac{1}{2}$ and $f(0) = -\frac{1}{2}$, we know the two roots are contained withing the interval $[-1, 1]$.
## 1b
$$
\begin{align}
g(x) &= x - x^2 + \frac{1}{2} \\
&= -(x^2-x) + \frac{1}{2} \\
&= -\left( x^2 - x + \frac{1}{4} - \frac{1}{4} \right) + \frac{1}{2} \\
&= -\left( x - \frac{1}{2} \right)^2 + \frac{3}{4}
\end{align}
$$
Looking at condition 2, $g'(x) = 1 - 2x$
$|g'(x)| < 1$ when $0 < x < 1$. As we want to consider the maximum guaranteed interval of convergence we will start with this interval.
For condition 1, $g(0) = \frac{1}{2}$, $g(1) = \frac{1}{2}$ We know that the vertex of this downward opening parabola (the highest point for $g(x)$) is at $x = \frac{1}{2}, g(x) = \frac{3}{4}$, thus $0 < x < 1 \implies 0 < \frac{1}{2} < g(x) < \frac{3}{4} < 1$.
We get $(0, 1)$ as a maximum open interval for convergence.
However, since we are iterating in a floating point system, such an open interval does not exist; a corresponding closed interval is : $[\epsilon, 1-\epsilon]$ where $\epsilon$ is machine epsilon.
Condition 2 for the theorem clearly still holds for this interval.
For condition 1,
$$
\begin{align}
\epsilon \le x \le 1 - \epsilon &\implies \epsilon - \frac{1}{2} \le x - \frac{1}{2} \le \frac{1}{2} - \epsilon \\
&\implies 0 \le \left( x - \frac{1}{2} \right)^2 \le \left( \frac{1}{2} - \epsilon \right)^2 =\frac{1}{4} - \epsilon + \epsilon^2 \\
&\implies  0 \ge -\left( x - \frac{1}{2} \right)^2 \ge -\frac{1}{4} + \epsilon - \epsilon^2 \\
&\implies \frac{3}{4} \ge -\left( x - \frac{1}{2} \right)^2 +\frac{3}{4} \ge \frac{1}{2} + \epsilon - \epsilon^2
\end{align}
$$
Thus condition 1 holds assuming $\epsilon$ is small and therefore this is the maximum interval of guaranteed convergence with only the Fixed Point Theorem.
## 1c
```python
def get_machine_epsilon():
	machine_epsilon = 1
	while 1 + (machine_epsilon / 2.0) != 1:
		machine_epsilon = machine_epsilon / 2.0
	return machine_epsilon

def f(x):
	return x**2 - 0.5
	
def g(x):
	return x - x**2 + 0.5

eps = get_machine_epsilon()

def fixed_point_iteration(x):
	prev = None
	k = 0
	print("|", "k ", "|", "x_{k-1} ", "|", "x_k ", "|")
	print("|----|--------------------|--------------------|")
	print("|", f"{k:02d}", "|", " ", "|", f"{x:.16f}", "|")
	while (k == 0 or abs(x-prev) > eps or abs(f(x)) > eps):
		prev = x
		x = g(x)
		k += 1
		print("|", f"{k:02d}", "|", f"{prev:.16f}", "|", f"{x:.16f}", "|")

fixed_point_iteration(0.7071)
```

| $k$  | $x_{k-1}$            | $x_k$ |
|---|---------------|--------------------------|
| 00 |                    | 0.7071000000000000 |
| 01 | 0.7071000000000000 | 0.7071095900000000 |
| 02 | 0.7071095900000000 | 0.7071056177300319 |
| 03 | 0.7071056177300319 | 0.7071072631046619 |
| 04 | 0.7071072631046619 | 0.7071065815692964 |
| 05 | 0.7071065815692964 | 0.7071068638706803 |
| 06 | 0.7071068638706803 | 0.7071067469376515 |
| 07 | 0.7071067469376515 | 0.7071067953729036 |
| 08 | 0.7071067953729036 | 0.7071067753103663 |
| 09 | 0.7071067753103663 | 0.7071067836205415 |
| 10 | 0.7071067836205415 | 0.7071067801783543 |
| 11 | 0.7071067801783543 | 0.7071067816041549 |
| 12 | 0.7071067816041549 | 0.7071067810135689 |
| 13 | 0.7071067810135689 | 0.7071067812581976 |
| 14 | 0.7071067812581976 | 0.7071067811568691 |
| 15 | 0.7071067811568691 | 0.7071067811988407 |
| 16 | 0.7071067811988407 | 0.7071067811814555 |
| 17 | 0.7071067811814555 | 0.7071067811886567 |
| 18 | 0.7071067811886567 | 0.7071067811856739 |
| 19 | 0.7071067811856739 | 0.7071067811869094 |
| 20 | 0.7071067811869094 | 0.7071067811863976 |
| 21 | 0.7071067811863976 | 0.7071067811866096 |
| 22 | 0.7071067811866096 | 0.7071067811865218 |
| 23 | 0.7071067811865218 | 0.7071067811865581 |
| 24 | 0.7071067811865581 | 0.7071067811865431 |
| 25 | 0.7071067811865431 | 0.7071067811865493 |
| 26 | 0.7071067811865493 | 0.7071067811865468 |
| 27 | 0.7071067811865468 | 0.7071067811865479 |
| 28 | 0.7071067811865479 | 0.7071067811865474 |
| 29 | 0.7071067811865474 | 0.7071067811865476 |
## 1d
Looking at the table, it seems to be linearly convergent.
# Q2
## 2a
![[Drawing 2023-11-14 09.56.18.excalidraw]]
Similar to in 1a, $f(-2) = f(2) = 2, f(0) = 2$, we know that the roots are contained within the interval $[-2, 2]$
## 2b
$$
\begin{align}
g(x) &= x - x^2 + 2 \\
&= -(x^2-x) + 2 \\
&= -\left( x^2 - x + \frac{1}{4} - \frac{1}{4} \right) + 2 \\
&= -\left( x - \frac{1}{2} \right)^2 + \frac{9}{4}
\end{align}
$$
$g'(x) = 1 - 2x$ As the derivative is the same as in Q1, we can draw the same potential range of $0 < x < 1$
However, notice that $g(0) = g(1) = 2, g\left( \frac{1}{2} \right) = \frac{9}{4}$. Since $g(x)$ is still a downward opening parabola with the vertex at $x = \frac{1}{2}$, we have that for $0 < x < 1 \implies 2 < g(x) < \frac{9}{4}$ which does not match the conditions for the Fixed Point Theorem.
As we have exhausted the potential ranges, we have identified that there is no guaranteed interval of convergence that can be derived from the Fixed Point Theorem.
## 2c
Unable to complete as there is no suitable range identified in 2b
## 2d
Unable to complete as 2c cannot be completed
# Q3
$f(x) = x - e^{-x}$
$g(x) = e^{-x}$
From lecture, we found the interval $[0.1, 1]$ that satisfies as a guaranteed interval of convergence:
1. $\forall x \in [0.1, 1], g(x) \in [0.1, 1]$ since $g(x)$ is monotonically decreasing (as $g'(x) = -e^{-x} < 0, \forall x$) and $g(0.1) \approx 0.9048, g(1) = 0.3679$
2. $|g'(x)| = e^{-x} = g(x) < 1, \forall x \in [0.1, 1]$ which can be seen with the same reasoning as above
Now let us consider the three cases as additional analysis when $0 \le x \le 0.1$, $x < 0$ and $x > 1$.

**Case 1: $0 \le x \le 0.1$**
In this case, $g(0) = 1, g(0.1) \approx 0.9048$ and since $g(x)$ is monotonically decreasing, $0 \le x \le 0.1 \implies 0.9048 \le g(x) \le 1$ which is inside our guaranteed interval of convergence after $1$ iteration.

**Case 2: $x > 1$**
In this case, we have the following bound: $0 < g(x) < g(1) \approx 0.3679$ since $g(x)$ is monotonically decreasing and can never reach $0$.
Thus if we were to use an initial starting point where $x_0 > 1$ then, $0 < x_1 \lessapprox 0.3679$ and $x_1$ is in our guaranteed interval of convergence.

**Case 3: $x < 0$**
In this case, $g(x) > 1$. Notice that if we start with $x_0 < 0$ then $x_1 > 1$ and by our initial found interval and Case 1, we can yet again guarantee convergence.

Therefore, the maximum guaranteed interval of convergence is all real numbers $\mathbb{R} = (-\infty, \infty)$, or in other words, it will converge no matter the start point $x_0$.
# Q4
We have our initial function to estimate for $f_0(y) = \sqrt[n]{y}$.
To turn this into a root finding problem, we can create a new function to find roots for which will equivalently solve the original function:
$$
f(x) = x^n - y = 0 \implies x^n = y \implies x = \sqrt[n]{y}
$$
This uses the assumption that $y > 0$ and we are finding the real roots.
For the case of $y = 0$, we already know that $f_0(0) = 0, \forall n$. 
We can then use Newton's Method, which we proved in lecture to be (at least) quadratically-convergent.
$$
f'(x) = nx^{n-1}
$$
$$
\begin{align}
g(x) &= x - \frac{f(x)}{f'(x)} \\
&= x - \frac{x^n - y}{n x^{n-1}} \\
%% &= x - \frac{x}{n} + \frac{y}{n x^{n-1}} %%
\end{align}
$$
Which leads to the fixed point iteration:
$$
x_{k+1} = x_k - \frac{x_k^n - y}{n x_k^{n-1}}
$$
# Q5
## Show Newton's method converges linearly
We want to show that $g'(\tilde{x}) \ne 0$.
We know the following:
$$
\begin{align}
f(x) &= (x - \tilde{x})^2 h(x) \\
f'(x) &= 2(x - \tilde{x}) h(x) + (x - \tilde{x})^2 h'(x) \\
f''(x) &= 2 h(x) + 2(x - \tilde{x}) h'(x) + 2(x - \tilde{x})h'(x) + (x - \tilde{x})^2 h''(x)
\end{align}
$$
$$
\begin{align}
g'(x) &= \frac{f(x) f''(x)}{(f'(x))^2} & \text{from lecture} \\ \\
&= \frac{(x - \tilde{x})^2 h(x) \left[ 2 h(x) + 2(x - \tilde{x}) h'(x) + 2(x - \tilde{x})h'(x) + (x - \tilde{x})^2 h''(x) \right]}{\left[ 2(x - \tilde{x}) h(x) + (x - \tilde{x})^2 h'(x) \right]^2} \\
&= \frac{(x - \tilde{x})^2 h(x) \left[ 2 h(x) + 2(x - \tilde{x}) h'(x) + 2(x - \tilde{x})h'(x) + (x - \tilde{x})^2 h''(x) \right]}{4(x-\tilde{x})^2[h(x)]^2 + 4(x-\tilde{x})^3h(x)h'(x) + (x-\tilde{x})^4[h'(x)]^2} \\
&= \frac{h(x) \left[ 2 h(x) + 2(x - \tilde{x}) h'(x) + 2(x - \tilde{x})h'(x) + (x - \tilde{x})^2 h''(x) \right]}{4[h(x)]^2 + 4(x-\tilde{x})h(x)h'(x) + (x-\tilde{x})^2[h'(x)]^2} \\
g'(\tilde{x}) &= \frac{h(\tilde{x}) [2 h(\tilde{x})]}{4[h(\tilde{x})]^2} \\
&= \frac{1}{2} & \text{since } h(\tilde{x}) \ne 0
\end{align}
$$
Therefore by the Rate of Convergence Theorem, Newton's method converges linearly.
## Show modified iteration converges at least quadratically
We want to show that for the modified iteration $g'(\tilde{x}) = 0$.
We know the following:
$$
\begin{align}
f(x) &= (x - \tilde{x})^2 h(x) \\
f'(x) &= 2(x - \tilde{x}) h(x) + (x - \tilde{x})^2 h'(x) \\
f''(x) &= 2 h(x) + 2(x - \tilde{x}) h'(x) + 2(x - \tilde{x})h'(x) + (x - \tilde{x})^2 h''(x)
\end{align}
$$
$$
g(x) = x - 2\frac{f(x)}{f'(x)}
$$
$$
\begin{align}
g'(x) &= 1 - 2\left( \frac{(f'(x))^2 - f(x) f''(x)}{(f'(x))^2} \right) \\
&= \frac{2f(x)f''(x) - (f'(x))^2}{(f'(x))^2} \\
&= \frac{2f(x)f''(x)}{(f'(x))^2} - \frac{(f'(x))^2}{(f'(x))^2}
\end{align}
$$
$$
\begin{align}
g'(\tilde{x}) &= \frac{2f(\tilde{x})f''(\tilde{x})}{(f'(\tilde{x}))^2} - \frac{(f'(\tilde{x}))^2}{(f'(\tilde{x}))^2} \\
&= 2\left( \frac{1}{2} \right) - \frac{(f'(\tilde{x}))^2}{(f'(\tilde{x}))^2} & \text{using previous part} \\
\end{align}
$$
Notice that $f'(\tilde{x}) = 0$ so $\frac{(f'(\tilde{x}))^2}{(f'(\tilde{x}))^2}$ is indeterminant.
Let us use L'Hopital's rule:
$$
\lim_{x \rightarrow \tilde{x}} \frac{(f'(x))^2}{(f'(x))^2} =
\lim_{x \rightarrow \tilde{x}} \frac{f''(x)}{f''(x)} = \frac{f''(\tilde{x})}{f''(\tilde{x})} = \frac{2h(x)}{2h(x)} = 1
$$
Therefore
$$
g'(\tilde{x}) = 2\left( \frac{1}{2} \right) - 1 = 0
$$
By the Rate of Convergence Theorem, this modified Newton's Method converges at least quadratically.
