# Q1
## 1a
![[Drawing 2023-11-13 18.58.15.excalidraw]]
Since $f(-1) = f(1) = \frac{1}{2}$ and $f(0) = -\frac{1}{2}$, we know the two roots are contained withing the interval $[-1, 1]$.
## 1b
$$
\begin{align}
g(x) &= x - x^2 + \frac{1}{2} \\
&= -(x^2-x) + \frac{1}{2} \\
&= -\left( x^2 - x + \frac{1}{4} - \frac{1}{4} \right) + \frac{1}{2} \\
&= -\left( x - \frac{1}{2} \right)^2 + \frac{3}{4}
\end{align}
$$
$g'(x) = 1 - 2x$
$|g'(x)| < 1$ when $0 < x < 1$. As we want to consider the maximum guaranteed interval of convergence we will start with this interval.
$g(0) = \frac{1}{2}$, $g(1) = \frac{1}{2}$ We know that the vertex of this downward opening parabola (the highest point for $g(x)$) is at $x = \frac{1}{2}, g(x) = \frac{3}{4}$, thus for $0 < x < 1, 0 < \frac{1}{2} < g(x) < \frac{3}{4} < 1$.
Thus we get the following maximum open interval $(0, 1)$ for convergence using only the Fixed Point Theorem.
However, we need a closed interval and since we are iterating in a floating point system, we can get the following closed interval: $[\epsilon, 1-\epsilon]$ where $\epsilon$ is machine epsilon.
## 1c
```python
def get_machine_epsilon():
	machine_epsilon = 1
	while 1 + (machine_epsilon / 2.0) != 1:
		machine_epsilon = machine_epsilon / 2.0
	return machine_epsilon

def f(x):
	return x**2 - 0.5
	
def g(x):
	return x - x**2 + 0.5

eps = get_machine_epsilon()

def fixed_point_iteration(x):
	prev = None
	k = 0
	print("|", "k ", "|", "x_{k-1} ", "|", "x_k ", "|")
	print("|----|--------------------|--------------------|")
	print("|", f"{k:02d}", "|", " ", "|", f"{x:.16f}", "|")
	while (k == 0 or abs(x-prev) > eps or abs(f(x)) > eps):
		prev = x
		x = g(x)
		k += 1
		print("|", f"{k:02d}", "|", f"{prev:.16f}", "|", f"{x:.16f}", "|")

fixed_point_iteration(0.7071)
```

| $k$  | $x_{k-1}$            | $x_k$ |
|---|---------------|--------------------------|
| 00 |                    | 0.7071000000000000 |
| 01 | 0.7071000000000000 | 0.7071095900000000 |
| 02 | 0.7071095900000000 | 0.7071056177300319 |
| 03 | 0.7071056177300319 | 0.7071072631046619 |
| 04 | 0.7071072631046619 | 0.7071065815692964 |
| 05 | 0.7071065815692964 | 0.7071068638706803 |
| 06 | 0.7071068638706803 | 0.7071067469376515 |
| 07 | 0.7071067469376515 | 0.7071067953729036 |
| 08 | 0.7071067953729036 | 0.7071067753103663 |
| 09 | 0.7071067753103663 | 0.7071067836205415 |
| 10 | 0.7071067836205415 | 0.7071067801783543 |
| 11 | 0.7071067801783543 | 0.7071067816041549 |
| 12 | 0.7071067816041549 | 0.7071067810135689 |
| 13 | 0.7071067810135689 | 0.7071067812581976 |
| 14 | 0.7071067812581976 | 0.7071067811568691 |
| 15 | 0.7071067811568691 | 0.7071067811988407 |
| 16 | 0.7071067811988407 | 0.7071067811814555 |
| 17 | 0.7071067811814555 | 0.7071067811886567 |
| 18 | 0.7071067811886567 | 0.7071067811856739 |
| 19 | 0.7071067811856739 | 0.7071067811869094 |
| 20 | 0.7071067811869094 | 0.7071067811863976 |
| 21 | 0.7071067811863976 | 0.7071067811866096 |
| 22 | 0.7071067811866096 | 0.7071067811865218 |
| 23 | 0.7071067811865218 | 0.7071067811865581 |
| 24 | 0.7071067811865581 | 0.7071067811865431 |
| 25 | 0.7071067811865431 | 0.7071067811865493 |
| 26 | 0.7071067811865493 | 0.7071067811865468 |
| 27 | 0.7071067811865468 | 0.7071067811865479 |
| 28 | 0.7071067811865479 | 0.7071067811865474 |
| 29 | 0.7071067811865474 | 0.7071067811865476 |
## 1d
Looking at the table, it seems to be linearly convergent.
# Q2
## 2a
![[Drawing 2023-11-14 09.56.18.excalidraw]]
Similar to in 1a, $f(-2) = f(2) = 2, f(0) = 2$, we know that the roots are contained within the interval $[-2, 2]$
## 2b
$$
\begin{align}
g(x) &= x - x^2 + 2 \\
&= -(x^2-x) + 2 \\
&= -\left( x^2 - x + \frac{1}{4} - \frac{1}{4} \right) + 2 \\
&= -\left( x - \frac{1}{2} \right)^2 + \frac{9}{4}
\end{align}
$$
$g'(x) = 1 - 2x$ As the derivative is the same as in Q1, we can draw the same potential range of $0 < x < 1$
However, notice that $g(0) = g(1) = 2, g\left( \frac{1}{2} \right) = \frac{9}{4}$. Since $g(x)$ is still a downward opening parabola with the vertex at $x = \frac{1}{2}$, we have that for $0 < x < 1, 2 < g(x) < \frac{9}{4}$ which does not match the conditions for the Fixed Point Theorem.
As we have exhausted the potential ranges, we have identified that there is no guaranteed interval of convergence that can be derived from the Fixed Point Theorem.
## 2c
Unable to complete as there is no suitable range identified in 2b
## 2d
Unable to complete as 2c cannot be completed
# Q3
$f(x) = x - e^{-x}$
$g(x) = e^{-x}$
From lecture, we found the interval $[0.1, 1]$ that satisfies as a guaranteed interval of convergence:
1. $\forall x \in [0.1, 1], g(x) \in [0.1, 1]$ since $g(x)$ is monotonically decreasing and $g(0.1) \approx 0.9048, g(1) = 0.3679$
2. $|g'(x)| = e^{-x} = g(x) < 1, \forall x \in [0.1, 1]$ which can be seen with the same reasoning as above
First notice that we can actually expand the interval to be $(0, 1]$ since $g(0) = 1$.
Now let us consider the two cases when $x \le 0$ and $x > 1$.

**Case 1: $x > 1$**
In this case, we have the following bound: $0 < g(x) < g(1) \approx 0.3679$ since $g(x)$ is monotonically decreasing and can never reach $0$.
Thus if we were to use an initial starting point where $x_0 > 1$ then, $0 < x_1 \lessapprox 0.3679$ and $x_1$ is in our guaranteed interval of convergence.

**Case 2: $x \le 0$**
In this case, $g(x) \ge 1$. Notice that if we start with $x_0 \le 0$ then $x_1 \ge 1$ and by our initial found interval and Case 1, we can yet again guarantee convergence.

Therefore, the maximum guaranteed interval of convergence is all real numbers $(-\infty, \infty)$, or in other words, it will converge no matter the start point $x_0$.
# Q4
We have our initial function to estimate for $f_0(y) = \sqrt[n]{y}$.
To turn this into a root finding problem, we can create a new function to find roots for which will equivalently solve the original function:
$$
f(x) = x^n - y = 0 \implies x^n = y \implies x = \sqrt[n]{y}
$$
We can then use Newton's Method, which we proved in lecture to be quadratically-convergent.
$$
f'(x) = nx^{n-1}
$$
$$
\begin{align}
g(x) &= x - \frac{f(x)}{f'(x)} \\
&= x - \frac{x^n - y}{n x^{n-1}} \\
%% &= x - \frac{x}{n} + \frac{y}{n x^{n-1}} %%
\end{align}
$$
Which leads to the fixed point iteration:
$$
x_{k+1} = x_k - \frac{x_k^n - y}{n x_k^{n-1}}
$$
Note that Newton's method will fail for $x = 0$, but we already know that $f_0(0) = 0$ so we can ignore this case.
# Q5
## Show Newton's method converges linearly
We want to show that $g'(\tilde{x}) \ne 0$.
$$
\begin{align}
f(x) &= (x - \tilde{x})^2 h(x) \\
f'(x) &= 2(x - \tilde{x}) h(x) + (x - \tilde{x})^2 h'(x) \\
f''(x) &= 2 h(x) + 2(x - \tilde{x}) h'(x) + 2(x - \tilde{x})h'(x) + (x - \tilde{x})^2 h''(x)
\end{align}
$$
$$
\begin{align}
g'(x) &= \frac{f(x) f''(x)}{(f'(x))^2} & \text{from lecture} \\ \\
&= \frac{(x - \tilde{x})^2 h(x) \left[ 2 h(x) + 2(x - \tilde{x}) h'(x) + 2(x - \tilde{x})h'(x) + (x - \tilde{x})^2 h''(x) \right]}{\left[ 2(x - \tilde{x}) h(x) + (x - \tilde{x})^2 h'(x) \right]^2} \\
g'(\tilde{x}) &= \frac{h(\tilde{x}) \left[ 2 h(\tilde{x}) + 2(x - \tilde{x}) h'(x) + 2(x - \tilde{x})h'(x) + (x - \tilde{x})^2 h''(x) \right]}{\left[ 2(x - \tilde{x}) h(x) + (x - \tilde{x})^2 h'(x) \right]^2}
\end{align}
$$
## Show modified iteration converges at least quadratically
We want to show that for the modified iteration $g'(\tilde{x}) = 0$.