---
tags:
  - CSCC37
  - C37Assignment
---
## 1a
$L_k$ has the following form:
$$
L_k = 
\begin{bmatrix}
1 & \cdots & 0 & 0 & \cdots & 0 \\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
0 & \cdots & 1 & 0 & \cdots & 0 \\
0 & \cdots & -l_{k+1} & 1 & \cdots & 0 \\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
0 & \cdots & -l_{n} & 0 & \cdots & 1 \\
\end{bmatrix}
$$
Consider the following:
$$
m_k =
\begin{bmatrix}
0 \\
\vdots \\
0 \\
l_{k+1} \\
\vdots \\
l_n
\end{bmatrix},
e_k =
\begin{bmatrix}
0 \\
\vdots \\
1 \\
0 \\
\vdots \\
0
\end{bmatrix}
=
\text{kth column of Identity matrix}
$$
$$
m_k e_k^{T} =
\begin{bmatrix}
0 & \cdots & 0 & 0 & \cdots & 0 \\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
0 & \cdots & 0 & 0 & \cdots & 0 \\
0 & \cdots & l_{k+1} & 0 & \cdots & 0 \\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
0 & \cdots & l_{n} & 0 & \cdots & 0 \\
\end{bmatrix}
,
(m_k e_k^{T})_{ij} =
\begin{cases}
l_i & \text{for } i = k+1 \ldots n, j = k \\
0 & \text{otherwise}
\end{cases}
$$
It is clearly evident that $L_k = I - m_k e_k^{T}$ after seeing the above form, thus (1) is shown.

For (2), $e_i^{T} m_k = e_i \cdot m_k = 0, i = 1, 2, \ldots, k$ since the first $k$ rows of $m_k$ are 0, which aligns with where the 1 element is in each $e_i$ and similarly, the 0 elements in $e_i$ align with the multipliers in $m_k$. Thus all multiplications become 0.
## 1b
Consider the following where $L_k^{-1} = I + m_k e_k^{T}$.
$$
\begin{align}
L_k L_k^{-1} &= (I - m_k e_k^{T})(I + m_k e_k^{T}) \\
&= I - m_k e_k^{T} + m_k e_k^{T} - m_k e_k^{T} m_k e_k^{T} & \text{since $e_k^{T} m_k = 0$ from part 1a} \\
&= I
\end{align}
$$
Thus by definition of inverse, $I + m_k e_k^{T}$ is the inverse of $L_k$. Since the multipliers in $L_k$ are defined by $m_k e_k^{T}$, and we add this term in $L_n^{-1}$ instead of subtracting, it is the same as changing the sign of each multiplier.
## 1c
$$
\begin{align}
(L_k L_j)^{-1} &= L_j^{-1} L_k^{-1} \\
&= (I + m_j e_j^{T})(I + m_k e_k^{T}) \\
&= I + m_j e_j^{T} + m_k e_k^{T} + m_j e_j^{T} m_k e_k^{T} \\
&= I + m_j e_j^{T} + m_k e_k^{T} & \text{since $e_j^{T} m_k$ = 0 as $j < k$ using part (b)} \\
&= (I - (-m_j e_j^{T})) + (I - (-m_k e_k^{T})) - I
\end{align}
$$
which is exactly as wanted.
## 1d
First note that for single swap row permutation matrices, the square of it is the identity since it swaps two rows and then swaps the same two rows back (ie. $P_i^{2} = I$).
Additionally we know that pre-multiplying by $P_i$ swaps rows $i$ and $j$, as well, post-multiplying by $P_i$ swaps columns $i$ and $j$.

$$
\begin{align}
\tilde{L}_k &= P_i L_k P_i \\
&= P_i (I - m_k e_k^{T}) P_i \\
&= P_i I P_i - P_i m_k e_k^{T} P_i \\
&= I - P_i m_k e_k^{T} P_i & \text{since $P_i^2 = I$}
\end{align}
$$
From this, we can clearly see that the application of the permutation matrices will only affect the multipliers. $P_i m_k e_k^{T}$ contains the multipliers with $l_i$ and $l_j$ swapped. Then we post-multiply by $P_i$ for  $P_i m_k e_k^{T} P_i$ which swaps columns $i$ and $j$. Assuming that $i > k$, the only non-zero column in $P_i m_k e_k^{T}$ is the $k$th column, the swap of column $i$ and $j$ only swaps columns consisting entirely of zeros, essentially doing nothing. Thus $P_i m_k e_k^{T} P_i$ is just $m_k e_k^{T}$ with multipliers $i$ and $j$ swapped. Using this in the decomposed form of $\tilde{L}_k$ above, we have that it is $L_k$ with multipliers $i$ and $j$ swapped as wanted.

If it were that $i \le k$, then in the post-multiplication step may actually swap column $k$ with some $j$ in the case that $i = k$. This would result in the $k$th column of $\tilde{L}_k$ being all 0 and not our desired matrix form.
## 2a
Consider the following creation of each $\tilde{L}_i$ term.
$$
\begin{align} \\
L_4P_4L_3P_3L_2P_2L_1P_1A = U \\
L_4(P_4L_3P_4)P_4P_3L_2P_2L_1P_1A = U \\
L_4\tilde{L}_3P_4P_3L_2P_2L_1P_1A = U \\
L_4\tilde{L}_3(P_4P_3L_2P_3P_4)P_4P_3P_2L_1P_1A = U \\
L_4\tilde{L}_3\tilde{L}_2P_4P_3P_2L_1P_1A = U \\
L_4\tilde{L}_3\tilde{L}_2(P_4P_3P_2L_1P_2P_3P_4)P_4P_3P_2P_1A = U \\
L_4\tilde{L}_3\tilde{L}_2\tilde{L}_1P_4P_3P_2P_1A = U \\
\end{align}
$$
Thus we have that:
$$
\begin{align}
\tilde{L}_1 &= P_4P_3P_2L_1P_2P_3P_4 \\
\tilde{L}_1^{-1} &= (P_4P_3P_2L_1P_2P_3P_4)^{-1} \\
&= P_4^{-1}P_3^{-1}P_2^{-1}L_1^{-1}P_2^{-1}P_3^{-1}P_4^{-1} \\
&= P_4P_3P_2L_1^{-1}P_2P_3P_4 & \text{since } P_i^{-1} = P_i
\end{align}
$$
## 2b
$$
P_2 = 
\begin{bmatrix}
1 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 \\
0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 \\
0 & 1 & 0 & 0 & 0
\end{bmatrix}
, ~
P_3 = 
\begin{bmatrix}
1 & 0 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 \\
0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 1
\end{bmatrix}
, ~
P_4 = 
\begin{bmatrix}
1 & 0 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 \\
0 & 0 & 0 & 1 & 0
\end{bmatrix}
$$
$$
L_1^{-1} =
\begin{bmatrix}
      1 & 0 & 0 & 0 & 0 \\
-m_{21} & 1 & 0 & 0 & 0 \\
-m_{31} & 0 & 1 & 0 & 0 \\
-m_{41} & 0 & 0 & 1 & 0 \\
-m_{51} & 0 & 0 & 0 & 1
\end{bmatrix}
, ~
P_2L_1^{-1}P_2 =
\begin{bmatrix}
      1 & 0 & 0 & 0 & 0 \\
-m_{51} & 1 & 0 & 0 & 0 \\
-m_{31} & 0 & 1 & 0 & 0 \\
-m_{41} & 0 & 0 & 1 & 0 \\
-m_{21} & 0 & 0 & 0 & 1
\end{bmatrix}
$$
$$
P_3P_2L_1^{-1}P_2P_3 =
\begin{bmatrix}
      1 & 0 & 0 & 0 & 0 \\
-m_{51} & 1 & 0 & 0 & 0 \\
-m_{41} & 0 & 1 & 0 & 0 \\
-m_{31} & 0 & 0 & 1 & 0 \\
-m_{21} & 0 & 0 & 0 & 1
\end{bmatrix}
, ~
\tilde{L}_1^{-1} = P_4P_3P_2L_1^{-1}P_2P_3P_4 =
\begin{bmatrix}
      1 & 0 & 0 & 0 & 0 \\
-m_{51} & 1 & 0 & 0 & 0 \\
-m_{41} & 0 & 1 & 0 & 0 \\
-m_{21} & 0 & 0 & 1 & 0 \\
-m_{31} & 0 & 0 & 0 & 1
\end{bmatrix}
$$

## 3a
$$
A =
\begin{bmatrix}
1 & 0 & 0 & \dots & 0 & 4 \\
2 & 1 & 0 & \dots & 0 & 0 \\
0 & 2 & 1 & \dots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \dots & 1 & 0 \\
0 & 0 & 0 & \dots & 2 & 1 \\
\end{bmatrix}
$$
$$
L_{1} =
\begin{bmatrix}
1 & 0 & 0 & \dots & 0 & 0 \\
-2 & 1 & 0 & \dots & 0 & 0 \\
0 & 0 & 1 & \dots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \dots & 1 & 0 \\
0 & 0 & 0 & \dots & 0 & 1 \\
\end{bmatrix}
,~
L_{1}A =
\begin{bmatrix}
1 & 0 & 0 & \dots & 0 & 4 \\
0 & 1 & 0 & \dots & 0 & -8 \\
0 & 2 & 1 & \dots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \dots & 1 & 0 \\
0 & 0 & 0 & \dots & 2 & 1 \\
\end{bmatrix}
$$
$$
L_{2} =
\begin{bmatrix}
1 & 0 & 0 & \dots & 0 & 0 \\
0 & 1 & 0 & \dots & 0 & 0 \\
0 & -2 & 1 & \dots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \dots & 1 & 0 \\
0 & 0 & 0 & \dots & 0 & 1 \\
\end{bmatrix}
,~
L_{2}L_{1}A =
\begin{bmatrix}
1 & 0 & 0 & \dots & 0 & 4 \\
0 & 1 & 0 & \dots & 0 & -8 \\
0 & 0 & 1 & \dots & 0 & 16 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \dots & 1 & 0 \\
0 & 0 & 0 & \dots & 2 & 1 \\
\end{bmatrix}
$$
Continuing this pattern, we can see that $U = L_n L_{n-1} \dots L_2 L_1 A$ has the form
$$
u_{ij} = 
\begin{cases}
1 & \text{if } i = j; i = 1 \dots n, j = 1 \dots n \\
(-2)^{i+1} & \text{if } j = n; i = 1 \dots n-1 \\
(-2)^{n+1} + 1 & \text{if } j = n, i = n \\
0 & \text{otherwise}
\end{cases}
$$
Now we find the inverses of our Gauss Transforms by flipping signs on each multiplier and combining Gauss Transforms together by adding them.
$$
L_{1}^{-1} =
\begin{bmatrix}
1 & 0 & 0 & \dots & 0 & 0 \\
2 & 1 & 0 & \dots & 0 & 0 \\
0 & 0 & 1 & \dots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \dots & 1 & 0 \\
0 & 0 & 0 & \dots & 0 & 1 \\
\end{bmatrix}
,~
L_{2}^{-1} =
\begin{bmatrix}
1 & 0 & 0 & \dots & 0 & 0 \\
0 & 1 & 0 & \dots & 0 & 0 \\
0 & 2 & 1 & \dots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \dots & 1 & 0 \\
0 & 0 & 0 & \dots & 0 & 1 \\
\end{bmatrix}
,~
L_{1}^{-1} L_{2}^{-1} =
\begin{bmatrix}
1 & 0 & 0 & \dots & 0 & 0 \\
2 & 1 & 0 & \dots & 0 & 0 \\
0 & 2 & 1 & \dots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \dots & 1 & 0 \\
0 & 0 & 0 & \dots & 0 & 1 \\
\end{bmatrix}
$$
We continue this similar pattern to get $L$.
$$
l_{ij} = 
\begin{cases}
1 & \text{if } i = j; i = 1 \dots n, j = 1 \dots n \\
2 & \text{if } i = j + 1; i = 2 \dots n, j = 1 \dots n-1 \\ 
0 & \text{otherwise}
\end{cases}
$$
Which is basically $A$ with the top right $4$ replaced with a $0$.
## 3b
The largest value in magnitude in $\mathbb{R}_{10}(16, 2)$ is $0.\underbrace{99 \ldots 99}_{16} \cdot 10^{99}$. The largest value in magnitude involved in the factorization is the term $(-2)^{n+1} + 1$. Overflow happens when:
$$
\begin{align}
& |(-2)^{n+1} + 1| > 0.\underbrace{99 \ldots 99}_{16} \cdot 10^{99} \\
\implies& 2^{n+1} + 1 > 0.\underbrace{99 \ldots 99}_{16} \cdot 10^{99} & \text{since we only consider magnitude}\\
\implies& n > \log_{2}{\left( 0.\underbrace{99 \ldots 99}_{16} \cdot 10^{99} - 1\right)} - 1  \\
\implies& n > 327 & \text{since $n$ is an integer}
\end{align}
$$
Thus $A$ must have dimensions greater than $327$ for overflow to occur.
## 3c
The factorization is not stable as it will not produce correct values for matrices whose side dimension is greater than $327$.
## 4a
##### Getting $U$
$$
A =
\begin{bmatrix}
1 & 0 & 0 & \dots & 0 & 4 \\
2 & 1 & 0 & \dots & 0 & 0 \\
0 & 2 & 1 & \dots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \dots & 1 & 0 \\
0 & 0 & 0 & \dots & 2 & 1 \\
\end{bmatrix}
$$
$$
P_1 =
\begin{bmatrix}
0 & 1 & 0 & \dots & 0 & 0 \\
1 & 0 & 0 & \dots & 0 & 0 \\
0 & 0 & 1 & \dots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \dots & 1 & 0 \\
0 & 0 & 0 & \dots & 0 & 1 \\
\end{bmatrix}
, ~
P_1 A = 
\begin{bmatrix}
2 & 1 & 0 & \dots & 0 & 0 \\
1 & 0 & 0 & \dots & 0 & 4 \\
0 & 2 & 1 & \dots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \dots & 1 & 0 \\
0 & 0 & 0 & \dots & 2 & 1 \\
\end{bmatrix}
$$
$$
L_{1} =
\begin{bmatrix}
1 & 0 & 0 & \dots & 0 & 0 \\
-\frac{1}{2} & 1 & 0 & \dots & 0 & 0 \\
0 & 0 & 1 & \dots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \dots & 1 & 0 \\
0 & 0 & 0 & \dots & 0 & 1 \\
\end{bmatrix}
, ~
L_1 P_1 A =
\begin{bmatrix}
2 & 1 & 0 & \dots & 0 & 0 \\
0 & -\frac{1}{2} & 0 & \dots & 0 & 4 \\
0 & 2 & 1 & \dots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \dots & 1 & 0 \\
0 & 0 & 0 & \dots & 2 & 1 \\
\end{bmatrix}
$$
$$
P_2 =
\begin{bmatrix}
1 & 0 & 0 & \dots & 0 & 0 \\
0 & 0 & 1 & \dots & 0 & 0 \\
0 & 1 & 0 & \dots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \dots & 1 & 0 \\
0 & 0 & 0 & \dots & 0 & 1 \\
\end{bmatrix}
, ~
P_2 L_1 P_1 A = 
\begin{bmatrix}
2 & 1 & 0 & \dots & 0 & 0 \\
0 & 2 & 1 & \dots & 0 & 0 \\
0 & -\frac{1}{2} & 0 & \dots & 0 & 4 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \dots & 1 & 0 \\
0 & 0 & 0 & \dots & 2 & 1 \\
\end{bmatrix}
$$
$$
L_{2} =
\begin{bmatrix}
1 & 0 & 0 & \dots & 0 & 0 \\
0 & 1 & 0 & \dots & 0 & 0 \\
0 & \frac{1}{4} & 1 & \dots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \dots & 1 & 0 \\
0 & 0 & 0 & \dots & 0 & 1 \\
\end{bmatrix}
, ~
L_2 P_2 L_1 P_1 A =
\begin{bmatrix}
2 & 1 & 0 & \dots & 0 & 0 \\
0 & 2 & 1 & \dots & 0 & 0 \\
0 & 0 & \frac{1}{4} & \dots & 0 & 4 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \dots & 1 & 0 \\
0 & 0 & 0 & \dots & 2 & 1 \\
\end{bmatrix}
$$
$$
\vdots
$$
Notice that $P_i$ swaps rows $i$ and $i + 1$. $L_i$ is the identity but with element $L_i[i+1, i] = -(-\frac{1}{2})^i$.
$$
P_{n-1} =
\begin{bmatrix}
1 & 0 & 0 & \dots & 0 & 0 \\
0 & 1 & 0 & \dots & 0 & 0 \\
0 & 0 & 1 & \dots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \dots & 0 & 1 \\
0 & 0 & 0 & \dots & 1 & 0 \\
\end{bmatrix}
, ~
P_{n-1} L_{n-2} P_{n-1} \cdots L_1 P_1 A = 
\begin{bmatrix}
2 & 1 & 0 & \dots & 0 & 0 \\
0 & 2 & 1 & \dots & 0 & 0 \\
0 & 0 & 2 & \dots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \dots & 2 & 1 \\
0 & 0 & 0 & \dots & \left(-\frac{1}{2}\right)^{n-2} & 4 \\
\end{bmatrix}
$$
$$
L_{n-1} =
\begin{bmatrix}
1 & 0 & 0 & \dots & 0 & 0 \\
0 & 1 & 0 & \dots & 0 & 0 \\
0 & 0 & 1 & \dots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \dots & 1 & 0 \\
0 & 0 & 0 & \dots & \left(-\frac{1}{2}\right)^{n-1} & 1 \\
\end{bmatrix}
, ~
U =
\begin{bmatrix}
2 & 1 & 0 & \dots & 0 & 0 \\
0 & 2 & 1 & \dots & 0 & 0 \\
0 & 0 & 2 & \dots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \dots & 2 & 1 \\
0 & 0 & 0 & \dots & 0 & \left(-\frac{1}{2}\right)^{n-1} + 4 \\
\end{bmatrix}
$$
We have that $U$ can be represented as follows:
$$
u_{ij} = 
\begin{cases}
2 & \text{if } i = j; i = 1 \ldots n-1, j = 1 \ldots n-1 \\
1 & \text{if } i = j - 1; i = 1 \ldots n-1, j = 2 \ldots n \\
\left(-\frac{1}{2}\right)^{n-1} + 4 & \text{if } i = n, j = n \\
0 & \text{otherwise}
\end{cases}
$$
##### Getting $P$
$$
P = P_{n-1} P_{n-2} \cdots P_2 P_1
$$
Swap rows 1 and 2, then 2 and 3, then 3 and 4, ..., $n-1$ and $n$. This ends up moving row 1 to the bottom row $n$ and then shifting all other rows up 1.
$$
p_{ij} = 
\begin{cases}
1 & \text{if } i = j - 1; i = 1 \ldots n-1, j = 2 \ldots n \\
1 & \text{if } i = n, j = 1 \\
0 & \text{otherwise}
\end{cases}
$$
##### Getting $L$
Drawing from our conclusion in Question 2, we get $\tilde{L}_{i} = P_{n-1} \cdots P_{i+1} L_{i} P_{i+1} \cdots P_{n-1}$ and $\tilde{L}_{i}^{-1} = P_{n-1} \cdots P_{i+1} L_{i}^{-1} P_{i+1} \cdots P_{n-1}$
$$
L_{1}^{-1} =
\begin{bmatrix}
1 & 0 & 0 & \dots & 0 & 0 \\
\frac{1}{2} & 1 & 0 & \dots & 0 & 0 \\
0 & 0 & 1 & \dots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \dots & 1 & 0 \\
0 & 0 & 0 & \dots & 0 & 1 \\
\end{bmatrix}
, ~
L_{2}^{-1} =
\begin{bmatrix}
1 & 0 & 0 & \dots & 0 & 0 \\
0 & 1 & 0 & \dots & 0 & 0 \\
0 & -\frac{1}{4} & 1 & \dots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \dots & 1 & 0 \\
0 & 0 & 0 & \dots & 0 & 1 \\
\end{bmatrix}
, ~
\cdots
$$
$$
L_{n-1}^{-1} =
\begin{bmatrix}
1 & 0 & 0 & \dots & 0 & 0 \\
0 & 1 & 0 & \dots & 0 & 0 \\
0 & 0 & 1 & \dots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \dots & 1 & 0 \\
0 & 0 & 0 & \dots & -\left(-\frac{1}{2}\right)^{n-1} & 1 \\
\end{bmatrix}
$$
$$
\tilde{L}_{1}^{-1} =
\begin{bmatrix}
1 & 0 & 0 & \dots & 0 & 0 \\
0 & 1 & 0 & \dots & 0 & 0 \\
0 & 0 & 1 & \dots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \dots & 1 & 0 \\
\frac{1}{2} & 0 & 0 & \dots & 0 & 1 \\
\end{bmatrix}
, ~
\tilde{L}_{2}^{-1} =
\begin{bmatrix}
1 & 0 & 0 & \dots & 0 & 0 \\
0 & 1 & 0 & \dots & 0 & 0 \\
0 & 0 & 1 & \dots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \dots & 1 & 0 \\
0 & -\frac{1}{4} & 0 & \dots & 0 & 1 \\
\end{bmatrix}
, ~
\cdots
$$
$\tilde{L}_{i}^{-1}$ is $L_i^{-1}$ but with the multiplier moved to the bottom. With this we can get:
$$
L = \tilde{L}_{1}^{-1} \cdots \tilde{L}_{n-2}^{-1} L_{n-1}^{-1} = 
\begin{bmatrix}
1 & 0 & 0 & \dots & 0 & 0 \\
0 & 1 & 0 & \dots & 0 & 0 \\
0 & 0 & 1 & \dots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \dots & 1 & 0 \\
\frac{1}{2} & -\frac{1}{4} & \frac{1}{8} & \dots & -\left(-\frac{1}{2}\right)^{n-1} & 1 \\
\end{bmatrix}
$$
So $L$ can be represented as:
$$
l_{ij} = 
\begin{cases}
1 & \text{if } i = j; i = 1 \ldots n, j = 1 \ldots n \\
-\left(-\frac{1}{2}\right)^{n-1} & \text{if }i = n; j = 1 \ldots n-1 \\
0 & \text{otherwise}
\end{cases}
$$
## 4b
Overflow will never happen as our non-constant terms in our factorization only get exponentially smaller as $n$ increases.
## 4c
Instead of overflow, the $u_{nn}$ term may underflow with large enough $n$. However, at that point the term may be insignificant enough to essentially consider as 0, so the factorization is stable.
## 5a
We can use a modified version of our current method of using permutation matrices and Gauss transforms.
Each $P_i$ and $L_i$ will now be of dimension $m \times m$ and we will need $n$ of these permutation and Gauss Transform matrices. Each iteration will be the same as the current algorithm:
1. For the $k^{th}$ iteration, first look for a row $j$, $j > k$, with largest element in magnitude in column $k$, and then swap rows $j$ and $k$ in $m \times m$ identity matrix to form $P_k$.
2. Then construct $L_k$ with the corresponding multipliers after applying the permutation matrix to eliminate the $k^{th}$ term for all rows below the current row
Notice that this process is repeated $n$ times unlike the $n-1$ times in a $n \times n$ matrix since we now must perform this operation for the $n^{th}$ column to eliminate all terms below $n^{th}$ row.

## 5b
The only computationally expensive part of the algorithm is in the application of each Gauss Transform. In the $k^{th}$ iteration for applying Gauss Transform $L_k$, we must perform $n-k$ flops for each row and repeat this for $m-k$ rows. Notice that in iteration $k$, we do not need to use flops to zero out the terms below the $k^{th}$ row in the $k^{th}$ column. As there are $n$ iterations in total, we can represent the computational complexity as follows:
$$
\begin{align}
\sum_{k = 1}^{n} (n-k)(m-k)
&= \sum_{k = 1}^{n} nm - (n+m)k + k^2 \\
&= n^2m + \sum_{k = 1}^{n} k^2 - (n+m)\sum_{k = 1}^{n}k \\
&= n^2m + \frac{n(n+1)(2n+1)}{6} - \frac{n(n+1)(n+m)}{2} \\
&= \frac{6n^2m}{6} + \frac{2n^3 + 3n^2 + n}{6} - \frac{3n^3 + 3n^2 + 3n^2m + 3nm}{6} \\
&= \frac{-n^3 + 3n^2m - 3nm + n}{6} \text{ flops}
\end{align}
$$

## 5c
If the system has no exact solution, then the solution to the system would likely be the the vector $x$ that produces the lowest normed residual $r = b - Ax$ which is essentially the closest value to an "answer" we can get.
## 6

Do something to one of the two equations before combining them.
$$
\begin{cases}
A\hat{x} &= b - r \\
Ax &= b
\end{cases}
$$
Subtracting the two equations:
$$
\begin{align}
&\implies r = A(x - \hat{x}) \\
&\implies ||r|| = ||A(x - \hat{x})|| \le ||A||~||x - \hat{x}|| & \text{submultiplicative} \\
&\implies \frac{||r||}{||A||} \le ||x - \hat{x}|| & \text{ Equation 1}
\end{align}
$$
$$
\begin{align}
x &= A^{-1}b \\
\implies ||x|| &= ||A^{-1}b|| \le ||A^{-1}||~||b||  & \text{submultiplicative}\\
\implies \frac{1}{||x||} &= \frac{1}{||A^{-1}b||} \ge \frac{1}{||A^{-1}||~||b||} & \text{ Equation 2}
\end{align}
$$
Combining Equations 1 and 2:
$$
\frac{||x - \hat{x}||}{||x||} \ge \frac{||r||}{||A||~||A^{-1}||~||b||} = \frac{1}{cond(A)} \frac{||r||}{||b||}
$$
Thus we have our desired lower bound.